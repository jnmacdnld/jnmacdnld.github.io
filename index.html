<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>John Macdonald</title>
    <link rel="stylesheet" href="style.2.css" type="text/css">

    <link href="https://fonts.googleapis.com/css?family=Raleway|Source+Sans+Pro" rel="stylesheet">

    <script type='text/javascript' src='js/jquery.min.js'></script>
    <script type='text/javascript' src='js/jquery.mobile.customized.min.js'></script>
    <script type='text/javascript' src='js/jquery.easing.1.3.js'></script> 
    <script type='text/javascript' src='js/camera.min.js'></script> 

    <script type='text/javascript' src='js/responsive_youtube.js'></script>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>


    <link rel='stylesheet' id='camera-css'  href='css/camera.css' type='text/css' media='all'> 

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109652169-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-109652169-1');
    </script>
    <script type="text/javascript">
      $( document ).ready(function() {

        var width = $('#content').width();

        jQuery('#beam-camera').camera({
            width: width + 'px',
            height: parseInt(width * 0.75) + 'px',
            pagination: true,
            thumbnails: false,
            navigation: true,
            autoAdvance: false,
            loader: 'bar',
            fx: 'scrollHorz'
        });

        jQuery('#skynet-camera').camera({
            width: width + 'px',
            height: parseInt(width * 0.75) + 'px',
            pagination: true,
            thumbnails: false,
            navigation: true,
            autoAdvance: false,
            loader: 'bar',
            fx: 'scrollHorz'
        });

        jQuery('#thorloki-camera').camera({
            width: width + 'px',
            height: parseInt(width * 0.75) + 'px',
            pagination: true,
            thumbnails: false,
            navigation: true,
            autoAdvance: false,
            loader: 'bar',
            fx: 'scrollHorz'
        });
      });
    </script>
  </head>
  
  <body>
    <div class="content" id="content">
      <h1 class='title'>John Macdonald</h1>
      <span class='subtitle'>Super excited about robotics and working hard to bring the benefits of robots
        to people.</span><br>
      <a href="https://twitter.com/jnmacdnld">Twitter</a>
      &#183;
      <a href="https://www.linkedin.com/in/john-joseph-macdonald/">LinkedIn</a>
      <br>
      <hr>

      <p>These are some of the robots I've worked on recently, including some pictures and videos as well. Hope you enjoy!</p>
      <h2>Beam</h2>
      <h3>Robot</h3>
      <ul>
        <li>Type: Two-Wheeled Mobile Indoor Robot</li>
        <li>Application: Research platform for socially competent navigation</li>
        <li>Organization: <a href="http://rpal.cs.cornell.edu">Cornell University Robotic Personal Assistants Lab</a> (<a href="https://github.com/Cornell-RPAL">GitHub</a>)</li>
      </ul>
      <h3>Personal Involvement</h3>
      <ul>
        <li>Role: Undergraduate Research Assistant</li>
        <li>Timeframe: September 2017 – May 2018</li>
        <li>Highlights:
            <ul>
            <li>Working on improving frame-to-frame people correspondence for people tracker</li>
            <li>Ported all software from ROS Indigo to Kinetic, gaining knowledge and experience with ROS</li>
            <li>Setup all software on a laptop, devsing several workarounds for Linux compatibility issues</li>
            </ul>
        </li>
      </ul>
      <h3>Gallery</h3>
      <div class="camera_wrap fluid_container camera_azure_skin" id="beam-camera">
          <div data-src="images/beam_robot_2.jpeg">
                <div class="camera_caption fadeFromBottom">
                    The robot consists of a BeamPro telepresence robot, an Occam Stereo for 360 stereo vision, a forward and read Hokuyo lidar sensors for obstacle avoidance and navigation, and a chest-level forward facing lidar for 2D to 3D correspondence during people tracking. An externally mounted Alienware 15 laptop running ROS Kinetic and equipped with a NVIDIA GTX 1080 GPU is used as the main computer.
                </div>
          </div>
          <div data-src="images/beam_gui.png">
                <div class="camera_caption fadeFromBottom">
                    Custom GUI interface for a telepresence application demonstation. People are recognized in the image using a neural network. The user can click on a person in the video feed to interact with, and the robot will navigate to that person, face them, and maintain a socially acceptable distance, even as the person moves.
                </div>
          </div>
          <div data-src="images/beam_map_overhead.png">
                <div class="camera_caption fadeFromBottom">
                    Overhead view in rviz of the map created by gmapping using a Hokuyo lidar sensor.
                </div>
          </div>
          <div data-src="images/beam_map_3d.png">
                  <div class="camera_caption fadeFromBottom">
                    3d view in rviz of a different mapping run. The colored scans from the multiple lidar sensors (chassis forward, chassis rear, and chest) are more easily distinguishable. The chest-height lidar is used for 2D to 3D correspondence for the people tracker.
                  </div>
          </div>
      </div>

      <h2>Next-Generation iRobot Roomba</h2>
      <h3>Robot</h3>
      <ul>
        <li>Type: Two-Wheeled Mobile Indoor Robot</li>
        <li>Application: Robotic Vacuuming</li>
        <li>Organization: <a href="http://irobot.com">iRobot Corporation</a></li>
      </ul>
      <h3>Personal Involvement</h3>
      <ul>
        <li>Personal Role: Software Engineering Intern</li>
        <li>Timeframe: May 2017 – August 2017</li>
        <li>Highlights:
            <ul>
              <li> Designed and implemented sensing and control methods for a floor cleaning robot to localize and align to a wall, exploring options to make wall alignment more precise and robust to different wall surfaces</li>
              <li> Designed and implemented a visualization tool to evaluate the area covered and path taken by an floor cleaning robot, enabling quick evaluation of the coverage performance of the software</li>
              <li> Integrated visualization tool into the existing nightly automated test infrastructure, enabling daily evaluation of the progress of floor cleaning robot path planning and coverage algorithms</li>
              <li><a href="https://github.com/ros-drivers/prosilica_driver/pull/15">Submitted pull request to ros_drivers/prosilica_driver package, which was merged</a> (ROS was used for marker tracking of a visual marker placed on the robot.)</li>
            </ul>
 </li>
      </ul>
      <h3>Gallery</h3>
      <p>Sorry! Due to confidentiality agreements, I cannot share any media. I do highly recommend the product when it comes out, though!</p>

      <h2>Artemis and Apollo</h2>
      <h3>Robot</h3>
      <ul>
        <li>Type: Pair of Mobile Underwater Robots</li>
        <li>Application: International RoboSub 2017 Competition Entry (Won 1st place)</li> 
        <li>Organization: <a href="http://cuauv.org">Cornell University Autonomous Underwater Vehicle Team</a>  (<a href="https://github.com/cuauv">GitHub</a>)</li>
      </ul>
      <h3>Personal Involvement</h3>
      <ul>
        <li>Personal Role: Software Team Lead</li>
        <li>Timeframe: September 2016 – July 2017</li>
        <li>Highlights:
            <ul>
                <li>Lead a team of 12 undergraduate students to develop software for a robotic submarine; requirements for robot include computer vision, acoustic beacon localization, and manipulation</li>
                <li>Wrote software for tracking of an acoustic beacon given the phase difference on arrival of the beacon signal between three recievers</li>            
                <li>Implemented a sparse monocular visual odometery algorithm using OpenCV, with the goal of exploring low-cost localization options of the smaller vehicle</li>
            </ul>
      </ul>
      <p>
        After a strong first year on the team, I was chosen to be one of the
        two software team leads, overseeing 15 software team members developing
        all of the software running on the main computer including that for vehicle
        control, computer vision, acoustic localization, and high level state
        machine logic. I emphasized early integration and system-level testing, as our team had
        previously focused on task-level testing (such as, the "Torpedoes" task
        or an object sorting task), leading to undiscovered 
        failure modes when the vehicle executed sequences of RoboSub mission
        challenges. (One such failure mode, paired with an unduly strong
        assuption about course setup, directly lead to a sub-par performance 
        in the RoboSub 2016 Finals and our placing third rather than first.)

        Leading the software team was one the most challenging, if not the most
        challenging, experience I have had thus far. 
      </p>
    
      <p>
        I viewed our secondary vehicle as crucial to our success. By splitting tasks
        between the subs and testing in paralell, this second "minisub"
        would enable us to effectively double our testing time. During the
        fall, I spent time
        researching how to build a visual odometry algorithm in
        order to enable position estimation on our small vehicle, which lacked
        the sonar sensor traditionally required for underwater navigation. An
        initial implementation worked well on indoor test data, but struggled
        underwater, where patterns of dancing light from surface waves can cause false
        motion.
        
        During the spring, I shelved my visual odometry development in order to
        focus on bringing the new vehicles into a state where the core vehicle
        sensing and control was reliable and task development could be reasonably
        be carried out by our
        software team members who would be staying over
        the summer. In particular, I focused on writing software to compensate
        for noise in our acoustic sensors needed for localization. At the same
        time, I coordinated and worked on cross-subteam fixes to deal with
        minisub control instability eaused by (1) our magentometer detecting 
        EM from upgraded thrusters and
        (2) forward thrusters which exterted too much torque about the vehicle
        CM. priveleged to have such a great team. With the work the team did over
        the summer, we managed to place first in RoboSub 2017. We achieved a
        semi-finals score which was higher than the next best team by a factor
        of ten, which was truly incredible. Seeing both vehicles working
        flawlessly in paralell was a moment I will never forget.
      </p>

      <h3>Competition Video</h3>
      <p>Video produced for the competition, giving an overview of the team's entry, as per the competition requirements. The video won the "Best Video" award.
      <iframe width="800px" height="450px" src="https://www.youtube.com/embed/V-gOZzkITTU?rel=0" frameborder="0" allowfullscreen></iframe>

      <h3>Software Overview Video</h3>
      <p>Video of the Artemis AUV operating autonomously at the RoboSub competition, and in simulation. The robot has to complete an underwater obstable course, including sorting colored tubes, firing small projectiles at targets, ramming into colored buoys, and passing through a field goal.</p>
      <iframe width="800" height="600" src="https://www.youtube.com/embed/Jfacv7x9poY" frameborder="0" gesture="media" allowfullscreen></iframe>
      <h3>Indoor Visual Odometry Demo</h3>
      This is a demonstration of the visual odometery algorithm being tested in
      an indoor environment. Movement of reflections and shadows from waves on
      the surface of the water unfortunately limits the usefulness of the
      algorithm underwater.
      <iframe width="800" height="450" src="https://www.youtube.com/embed/FRSd12OvMlI" frameborder="0" gesture="media" allowfullscreen></iframe>

      <h2>Skynet Jr.</h2>
      <h3>Pesonal Involvement</h3>
      <ul>
        <li>Personal Role: Team Member</li>
        <li>Timeframe: January 2017 – May 2017</li>
      </ul>
      <h3>Robot</h3>
        <li>Type: Off-Road Autonomous Car</li>
        <li>Application: Class project which became a research platform</li>
        <li>Organization: Class project (formerly), Cornell University Robotic Personal Assistants Lab (currently)</li>
        <li>Highlights:
          <ul>
            <li>Integrated ROS with an RC car chassis and stereo camera to create
              a small self-driving car.</li>
          </ul>
        </li>
      </ul>
      <h3>Gallery</h3>
      <div class="camera_wrap fluid_container camera_azure_skin" id="skynet-camera">
          <div data-src="images/skynet_34.jpeg">
                <div class="camera_caption fadeFromBottom">Front view of the Skynet Jr. robot. A ZED stereo camera is used for obstacle detection and navigation. An NVIDIA Jetson TX1 serves as the main computer.
                </div>
          </div>
          <div data-src="images/skynet_front.jpeg">
                <div class="camera_caption fadeFromBottom">Front view.</div>
          </div>
          <div data-src="images/skynet_side.jpeg">
                <div class="camera_caption fadeFromBottom">Side view.</div>
          </div>
          <div data-src="images/pointcloud_cropped.png">
                <div class="camera_caption fadeFromBottom">Point cloud calculated by the ZED stereo camera being visualized by rviz.</div>
          </div>
          <div data-src="images/navigation_cropped.png">
                <div class="camera_caption fadeFromBottom">Visual odometery from the ZED and local map from the ROS navigation stack being visualized by rviz. The red object is a point cloud of a red sofa.</div>
          </div>
      </div>

      <h2>Thor and Loki</h2> <h3>Robot</h3>
      <ul>
        <li>Type: Pair of Mobile Underwater Robots</li>
        <li>Application: International RoboSub 2016 Competition Entry (Won 3rd place)</li> 
        <li>Organization: <a href="http://cuauv.org">Cornell University Autonomous Underwater Vehicle Team</a> (<a href="https://github.com/cuauv">GitHub</a>) </li>
      </ul>
      <h3>Personal Involvement</h3>
      <ul>
        <li>Personal Role: Software Team Member</li>
        <li>Timeframe: September 2015 – July 2016</li>
      </ul>
      <p>
        CUAUV was my first experience working on a relatively complex
        robotics project, an underwater robot with a 50-person team
        consisting of about 15 software team members. (I had competed in high
        school robotics, but I was leading a relatively small 3-person team.)
        The objective of CUAUV is 
        to produce a robot to compete in the international RoboSub
        competition. As a team member, I developed an interest
        in computer vision, researching topics such as graph-based image
        segmentation to enable object detection invariant to lighting
        conditions and visual egomotion in order to caculate velocity without
        an expensive sonar sensor. I implemented simple proof-of-concept prototype
        algorithms for both of these. 
      </p>
      <p>
        As the competition takes in July, some team members opt to stay
        on-campus over the summer to work on the robot. I opted to stay
        during Summer 2016 as a sophomre. I worked extensively on computer vision algorithms
        and high-level state machine and control logic to
        to complete the "Torpedoes" task at RoboSub 2016. The task consisted of
        identifing targets labeled with letters, removing a physical cover from a target, 
        and shooting plastic projectiles at targets with specified letters.
        (Video and images are shown below.) I spent nearly all of my time
        that summer writing computer vision code, testing and collecting logs in the pool,
        and revising and re-desgining code as needed to account for new failure
        modes, re-running against the collected log data, and finally
        re-testing in the pool.
      </p>
      <p>
        I had a fantastic experience attending RoboSub 2016, speaking with
        other teams about how they had approached the challenges and watching
        my code manuver the vehicle around the obstacle course. I was super
        excited when our team acheived third place.
      </p>

      <h3>Competition Video</h3>
<iframe width="800" height="400" src="https://www.youtube.com/embed/KB5b_M7Ps2Y" frameborder="0" allowfullscreen></iframe>
      <h3>Torpedoes Task Video</h3>
      <iframe width="800" height="400" src="https://www.youtube.com/embed/GOj6JVd1Q9E" frameborder="0" gesture="media" allowfullscreen></iframe>

      <h3>Gallery</h3>
      <div class="camera_wrap fluid_container camera_azure_skin" id="thorloki-camera">
          <div data-src="images/thor_crane_below_resize.jpg">
                <div class="camera_caption fadeFromBottom">Thor being lifted into the competition pool by the crane.</div>
          </div>
          <div data-src="images/torpedoes_vision_gui.png">
                <div class="camera_caption fadeFromBottom">Visualization of the
                torpedoes task algorithm executing in real-time.</div>
          </div>
          <div data-src="images/thor_debug_resize.png">
                <div class="camera_caption fadeFromBottom">Thor being debugged via the shared memory editor and plotter.</div>
          </div>
          <div data-src="images/thor_bins_sim_resize.png">
                <div class="camera_caption fadeFromBottom">Thor testing mission
                logic inside of the custom simulator.</div>
          </div>
          <div data-src="images/thor_sim_resize.png">
                <div class="camera_caption fadeFromBottom">Thor exploring the
                  simulator.</div>
          </div>

      </div>

    </div>
  </body>
</html>

