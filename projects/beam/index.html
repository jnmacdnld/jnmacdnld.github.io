<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8"> 
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title></title>
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/css/3-col-portfolio.css" type="text/css">
  <link rel="stylesheet" href="/css/bootstrap.min.css" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Raleway|Source+Sans+Pro" rel="stylesheet">

  <script type='text/javascript' src='/js/jquery.min.js'></script>
  <script type='text/javascript' src='/js/bootstrap.bundle.min.js'></script>
  <script type='text/javascript' src='/js/jquery.mobile.customized.min.js'></script>
  <script type='text/javascript' src='/js/jquery.easing.1.3.js'></script> 
  <script type='text/javascript' src='/js/camera.js'></script> 
  <script type='text/javascript' src='/js/carosel.js'></script> 

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <script type='text/javascript' src='/js/responsive_youtube.js'></script>

  
  


  <link rel='stylesheet' id='camera-css'  href='/css/camera.css' type='text/css' media='all'> 
  <link rel='stylesheet' href='/css/carosel.css' type='text/css' media='all'> 

  
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

  gtag('config', '');
  </script>
</head>



<body>
<div class="content" id="content">
<p><a href="/">Back to all Projects</a>

</p>
<br>



<h2>Beam</h2>


<h3>My Contribution Highlights</h3>
<ul>
  
  <li>Transformed the novel Social Momentum algorithm from equations and MatLab simulations to Python code running in real-time on a BeamPro robot with ROS Kinetic, enabling navigation with socially desirable collision avoidance behaviors</li>

  <li>Designed and implemented framework for socially aware navigation algorithms, enabling future comparison of arbitrary algorithms including OCRA, Social Force, and Social Momentum running in real-time on a robot</li>

  <li>Paper on comparison of social navigation algorithms using the framework accepted to HRI 2019</li>

</ul>



<div class="container-fluid media-gallery-container">
  <div id="myCarousel" class="carousel slide media-gallery"
    data-ride="carousel" data-interval=false>
    <div class="carousel-inner row w-100 mx-auto">
        
        <div class="carousel-item col-md-4
        
          active 
        
          ">
          <div class="card">
            
              <img class="card-img-top img-fluid" src="/images/beam_robot_2.jpeg"
            height=600>
            
            <div class="card-body">
              
              <p class="card-text">The robot consists of a BeamPro telepresence robot, an Occam Stereo for 360 stereo vision, a forward and read Hokuyo lidar sensors for obstacle avoidance and navigation, and a chest-level forward facing lidar for 2D to 3D correspondence during people tracking. An externally mounted Alienware 15 laptop running ROS Kinetic and equipped with a NVIDIA GTX 1080 GPU is used as the main computer.</p>
              
            </div>
          </div>
        </div>
        
        <div class="carousel-item col-md-4
        
          ">
          <div class="card">
            
              <img class="card-img-top img-fluid" src="/images/beam_map_overhead.png"
            height=600>
            
            <div class="card-body">
              
              <p class="card-text">Overhead view in rviz of the map created by gmapping using a Hokuyo lidar sensor.</p>
              
            </div>
          </div>
        </div>
        
        <div class="carousel-item col-md-4
        
          ">
          <div class="card">
            
              <img class="card-img-top img-fluid" src="/images/beam_map_3d.png"
            height=600>
            
            <div class="card-body">
              
              <p class="card-text">3d view in rviz of a different mapping run. The colored scans from the multiple lidar sensors (chassis forward, chassis rear, and chest) are more easily distinguishable.     The chest-height lidar is used for 2D to 3D correspondence for the people tracker.</p>
              
            </div>
          </div>
        </div>
        
    </div>

    <a class="carousel-control-prev" href="#myCarousel" role="button" data-slide="prev">
      <span class="carousel-control-prev-icon" aria-hidden="true"></span>
      <span class="sr-only">Previous</span>
    </a>
    <a class="carousel-control-next" href="#myCarousel" role="button" data-slide="next">
      <span class="carousel-control-next-icon" aria-hidden="true"></span>
      <span class="sr-only">Next</span>
    </a>
  </div>
</div>


<h3>Project Info</h3>
<ul>
  
    <li>Robot Type: Two-Wheeled Mobile Indoor Robot</li>
    <li>Application: Research platform for socially competent navigation</li>
  
  <li>Organization: 
    
       
        <a href="http://rpal.cs.cornell.edu"> 
      
      Cornell University Robotic Personal Assistants Lab
       
        </a> 
      

       
        (<a href="https://github.com/Cornell-RPAL">GitHub</a>)
      
    
  
  </li>
  
    <li>Personal Role: Undergraduate Research Assistant, September 2017 â€“ May 2018</li>
  
</ul>



<h3>More About My Experience</h3>
<p>
  <p>I spent my senior year of undergrad working in the Robotic Personal Asisstants
Lab on the socially competent navigation project. My role was to move the
project from a set of simulations running in Matlab to Python code running on a
ROS robot in real-time. I designed and implemented an architecure which enabled
modular swapping of
different person perception and social navigation algorithms in order to gauge
real-world performance.</p>
<p>In order to compare a range on algorithms, some which
assumed certain motion constraints and some which did not, we converged on
defining the action which an algorithm
would take as an arbitrary velocity vector, which the framework would convert
into wheel velocities using a feedback linearization approach. Additional
constraints e.g. velocity magnitude constraint could be enforced by the
framework
in order to suit the platform being used.</p>
<p>I also implemented the lab&rsquo;s novel Social Momentum algorithm inside this
framework, as well as Optimal Reciprocal Collision Avoidance (ORCA) using the RVO library in order to demonstrate the
interopability of the framework, as well as to enable comparison between
different algorithms.</p>
<p>The social navigation framework which I designed and implemented was used to
conduct experiments on social navigation, the results of which were summarised
in a paper accepted to HRI 2019.</p>


</p>


<p><a href="/">Back to all Projects</a>

</p>
<br>



    </div>
</body>

</html>

